# backend/test_core_logic.py

import os
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyMuPDFLoader  # PDF ë¡œë”
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- 1. ì„¤ì • (Ollama, ChromaDB) ---
PDF_FILE_PATH = "test.pdf"  # âš ï¸ backend í´ë”ì— ì¤€ë¹„í•œ PDF íŒŒì¼ ì´ë¦„
DB_PATH = "./data/vector_db" # ChromaDBë¥¼ ì €ì¥í•  ë¡œì»¬ ê²½ë¡œ
OLLAMA_BASE_URL = "http://127.0.0.1:11434"
OLLAMA_MODEL = "exaone3.5:2.4b"

# --- 2. Ollama ëª¨ë¸ ë° ì„ë² ë”© ë¡œë“œ ---
try:
    llm = Ollama(model=OLLAMA_MODEL, base_url=OLLAMA_BASE_URL)
    embeddings = OllamaEmbeddings(model=OLLAMA_MODEL, base_url=OLLAMA_BASE_URL)
    print("âœ… Ollama ì—°ê²° ì„±ê³µ.")
except Exception as e:
    print(f"ğŸš¨ Ollama ì—°ê²° ì‹¤íŒ¨! ğŸš¨")
    print(f"ì˜¤ë¥˜: {e}")
    print("í„°ë¯¸ë„ 1ë²ˆì—ì„œ 'ollama serve'ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit()

# --- 3. (í…ŒìŠ¤íŠ¸) ê°„ë‹¨í•œ LLM í˜¸ì¶œ ---
try:
    print("\n--- 1. ê°„ë‹¨í•œ LLM í˜¸ì¶œ í…ŒìŠ¤íŠ¸ ---")
    response = llm.invoke("ì•ˆë…•í•˜ì„¸ìš”, 1+1ì€ ë¬´ì—‡ì¸ê°€ìš”?")
    print(f"Ollama ì‘ë‹µ: {response}\n")
except Exception as e:
    print(f"ğŸš¨ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    exit()

# --- 4. RAG íŒŒì´í”„ë¼ì¸ (PDF ë¡œë“œ, ë¶„í• , ì €ì¥) ---
print("--- 2. RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ---")

if not os.path.exists(PDF_FILE_PATH):
    print(f"ğŸš¨ ì˜¤ë¥˜: '{PDF_FILE_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("backend í´ë”ì— í…ŒìŠ¤íŠ¸ìš© PDF íŒŒì¼ì„ ì¤€ë¹„í•˜ì„¸ìš”.")
    exit()

try:
    # 1. ë¡œë“œ (PDF)
    print(f"'{PDF_FILE_PATH}' ë¡œë“œ ì¤‘...")
    loader = PyMuPDFLoader(PDF_FILE_PATH)
    docs = loader.load()

    # 2. ë¶„í•  (Text Chunks)
    print("í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(docs)

    # 3. ì €ì¥ (ChromaDBì— ì„ë² ë”©í•˜ì—¬ ì €ì¥)
    print(f"'{DB_PATH}' ê²½ë¡œì— ChromaDB ìƒì„± ë° ì €ì¥ ì¤‘...")
    # (ì°¸ê³ : ì´ë¯¸ ì €ì¥í–ˆë‹¤ë©´ ì´ ì½”ë“œëŠ” ê¸°ì¡´ DBë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤)
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=DB_PATH # íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì €ì¥
    )
    
    # 4. ê²€ìƒ‰ê¸°(Retriever) ìƒì„±
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 3} # ê´€ë ¨ì„± ë†’ì€ 3ê°œ ì¡°ê° ê²€ìƒ‰
    )
    print("âœ… RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ.")

except Exception as e:
    print(f"ğŸš¨ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì‹¤íŒ¨: {e}")
    exit()

# --- 5. RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸ ---
print("\n--- 3. RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸ ---")

# (1) RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
template = """
ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI íŠœí„°ì…ë‹ˆë‹¤.
ì œì‹œëœ [ì»¨í…ìŠ¤íŠ¸] ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ [ì§ˆë¬¸]ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.

[ì»¨í…ìŠ¤íŠ¸]:
{context}

[ì§ˆë¬¸]:
{question}
"""
prompt = ChatPromptTemplate.from_template(template)

# (2) LangChain ì²´ì¸(Chain) êµ¬ì„±
#    {context} ì—ëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼, {question} ì—ëŠ” ì›ë˜ ì§ˆë¬¸ì„ ë„£ìŒ
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# (3) RAG ì‹¤í–‰
try:
    # âš ï¸ PDF ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”
    question = "PDF íŒŒì¼ì˜ í•µì‹¬ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?" 
    print(f"\n[RAG í…ŒìŠ¤íŠ¸ ì§ˆë¬¸]: {question}")
    
    response = chain.invoke(question)
    
    print(f"\n[RAG ì‘ë‹µ]:\n{response}\n")
    print("âœ… RAG í…ŒìŠ¤íŠ¸ ì„±ê³µ!")

except Exception as e:
    print(f"ğŸš¨ RAG ì‹¤í–‰ ì‹¤íŒ¨: {e}")